---
title: "5:49:28 PM - February 2, 2026"
date: 2026-02-02T22:49:28.822Z
timestamp: 1770072568822
---

## Project Notes

## Ingestion Flow Analysis - Complete Picture

### High-Level Flow: Source → Pipeline → Persistence

The ingestion process flows through:
1. **Source Creation** - Sources yield Documents with metadata
2. **IngestionPipeline** - LlamaIndex's pipeline runs transforms on documents
3. **PersistenceTransform** - Custom transform that writes to database
4. **FTS Indexing** - Full-text search updates happen during persistence

### Key Classes and Responsibilities

**DatasetIngestPipeline.ingest_dataset()** (lines 79-194):
- Entry point that orchestrates the entire flow
- Creates or retrieves dataset via DatasetService.create_or_update()
- Creates PersistenceTransform with dataset_id and force flag
- If force=True: clears cache AND deletes vectors via VectorStoreManager
- Runs pipeline with source.documents
- Collects stats from persist.stats

**IngestPipeline** (lines 222-684):
- Alternative implementation with more complete transformation chain
- Can use docstore caching with DocstoreStrategy (UPSERTS, UPSERTS_AND_DELETE, DUPLICATES_ONLY)
- Vector store integration for native embedding persistence

### Change Detection Mechanism (PersistenceTransform._process_node)

**Location**: catalog/transform/llama.py lines 472-565

The logic:
1. Extract path from node.metadata["relative_path"] or node.node_id
2. Compute content_hash = SHA256(body) via _compute_content_hash() (line 304)
3. Pre-fetch existing documents by path (lines 410-417)
4. For each node, check if document already exists:

**Decision Tree**:
```
if existing document found:
  if NOT force AND existing.content_hash == new_content_hash:
    # UNCHANGED CONTENT
    if existing.active == False:
      # Was soft-deleted, reactivate and update metadata
      existing.active = True
      fts.upsert(...)
      stats.updated += 1
    else:
      # Already active, truly no change
      stats.skipped += 1
  else:
    # CHANGED OR FORCE=TRUE
    existing.content_hash = new_content_hash
    existing.body = body
    existing.active = True
    fts.upsert(...)
    stats.updated += 1
else:
  # NEW DOCUMENT
  doc_repo.create(...)
  fts.upsert(...)
  stats.created += 1
```

### Content Hash Storage

**Document model** (catalog/store/models.py lines 169-236):
- `content_hash: str` field on Document (line 201)
- Indexed via `ix_documents_content_hash` (line 227)
- Stored as SHA256 hexdigest (64 chars)

### Force Flag Behavior

**In DatasetIngestPipeline.ingest_dataset()**:
- Lines 137-142: If force=True:
  - Clears pipeline docstore cache via `clear_cache(normalized_name)`
  - Deletes all vectors for dataset via `vector_manager.delete_by_dataset(normalized_name)`
  
**In PersistenceTransform._process_node()**:
- Line 505: If NOT force AND hashes match → skip
- Line 524: If force OR hash changed → update (regardless of hash)

### Key Metadata Fields

From node.metadata in PersistenceTransform._process_node():
- `relative_path` - Document path within dataset (PRIMARY KEY with parent_id)
- `etag` - Optional source etag for change detection
- `last_modified` - Optional source modification time
- `title` - Document title
- `description` - Document description
- `_ontology_meta` - Structured metadata from FrontmatterTransform
- Other fields are filtered and stored as JSON

### FTS Indexing

**FTSManager.upsert()** called for each document:
- Updates documents_fts table with full-text index
- Rows identified by doc.id (Database Document ID)
- Content indexed from node body

### Chunk Persistence (ChunkPersistenceTransform)

**Location**: catalog/transform/llama.py lines 600-874

Chunks from MarkdownNodeParser get:
- Stable node_id = `{content_hash}:{chunk_seq}`
- Metadata fields: source_doc_id, chunk_seq, chunk_pos, doc_id
- FTS5 upsert to chunks_fts table
- Content hash inherited from parent document

### Pipeline Caching (cache.py)

**persist_pipeline()** (lines 116-130):
- Saves pipeline's internal docstore to disk
- Docstore tracks document hashes for LlamaIndex deduplication
- Only called manually, NOT automatic in current flow

**clear_cache()** (lines 148-162):
- Removes entire cache directory (pipeline_storage/{dataset_name}/)
- Called when force=True

**Note**: The cache is NOT actively used in current flow because:
- IngestionPipeline created with empty SimpleDocumentStore() (line 161)
- Pipeline is NOT reloaded with cached state before running
- So docstore deduplication NOT happening at LlamaIndex level

### Critical Finding: Inactive Document Logic

Lines 507-521 in _process_node():
- If document exists but inactive (active=False), and content hash unchanged:
  - Still counts as updated, not skipped
  - Reactivates the document
  - Updates FTS
  - This is soft-delete recovery, not typical "unchanged" skip

### Summary of Change Detection

**Layer 1: Content-based** (PersistenceTransform)
- Hash computed from normalized body
- Compared against existing.content_hash
- If changed or force=True → update

**Layer 2: Source-based metadata** (Not currently used for decision)
- etag field stored but not compared
- last_modified field stored but not compared
- Could be added for optimization

**Layer 3: LlamaIndex docstore** (Not currently active)
- Could use UPSERTS strategy for automatic deduplication
- Currently bypassed by creating empty SimpleDocumentStore()

