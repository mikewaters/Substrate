# LLM Configuration
# Controls which LLM provider and model is used for document classification

# Provider: "anthropic" (default) or "openai-compatible" for local models via LM Studio, vLLM, or Ollama
export SUBSTRATE_LLM_PROVIDER=openai-compatible

# OpenAI-compatible endpoint URL (only used when provider=openai-compatible)
# Examples:
#   - LM Studio: http://localhost:1234/v1
#   - vLLM: http://localhost:8000/v1
#   - Ollama: http://localhost:11434/v1
export SUBSTRATE_LLM_OPENAI_BASE_URL=http://localhost:1234/v1

# Model name (varies by provider)
# For local models, use larger models capable of following instructions:
#   - Good options: neural-chat-7b, mistral-7b, neural-chat-7b-v3-1, llama2-13b
#   - Note: Very small models (<7B) may not follow JSON formatting reliably
export SUBSTRATE_LLM_MODEL_NAME=neural-chat-7b

# API key (can be dummy for local models like LM Studio)
# export SUBSTRATE_LLM_API_KEY=sk-local

# Alternative: Use Anthropic provider (default)
# export SUBSTRATE_LLM_PROVIDER=anthropic
# export SUBSTRATE_LLM_MODEL_NAME=claude-sonnet-4-20250514

# Temperature for generation (0.0-2.0, default: 0.7)
# SUBSTRATE_LLM_TEMPERATURE=0.7

# Maximum tokens for responses (default: 2048)
# SUBSTRATE_LLM_MAX_TOKENS=2048

# Database Configuration
# SUBSTRATE_DATABASE_DB_PATH=.data/ontology.db
# SUBSTRATE_DATABASE_ECHO=false
# SUBSTRATE_DATABASE_ENABLE_WAL=true

# Logging Configuration
# SUBSTRATE_LOGGING_LEVEL=INFO
# SUBSTRATE_LOGGING_STRUCTURED=true

# Application Environment
# SUBSTRATE_ENVIRONMENT=dev
