# Using LLMs for institutional-grade investigations in adversarial contexts

YouTube Nate B Jones 8 Ways to Use AI When Someone Is Trying to Screw You (Adversarial Prompting)

The methodology described consists of eight specific capabilities or principles that make Large Language Models (LLMs) uniquely powerful for conducting institutional-grade investigations in adversarial contexts \[1\]. These steps are designed to be followed methodically and build upon each other to overcome institutional information asymmetry \[2, 3\].

Here are the eight principles in detail:

### 1\. LLM Parse the Technical Framework

The first step is to have the LLM analyze the technical framework that institutions rely on to intimidate humans \[4\]. This includes reading documents such as Medicare billing rules, FDCPA statutes, IEP regulations, or insurance policy appendices \[4\]. These technical documents are often deliberately designed to be unreadable by people \[4\]. Because AI is trained on this jargon, it quickly decodes the rules, enabling the user to audit the institution's compliance with its own standards, even without prior subject matter expertise \[4\].

### 2\. Cross-reference Multiple Authority Sources

Use the LLM to cross-reference multiple authoritative standards simultaneously \[5\]. This is crucial because violations often hide in the gaps between different documents, such as checking if CPT codes were billed correctly against CMS bundling rules, Medicare fee schedules, and setting requirements \[5\]. Multi-document pattern recognition is extremely difficult for humans to handle but is something AI excels at \[5\].

### 3\. LLM Match Institutional Register

The LLM should draft correspondence that signals professionalism and understanding of the system \[6\]. This is known as matching the institutional register. The AI can draft documents using a formal register of English, legal terminology, cite regulatory citations, and appropriately measure escalation threats \[6\]. Institutions triage disputes based on their sophistication; by using the AI to write with a professional cadence, you signal understanding of the system, which pushes your dispute to the top and makes the institution more likely to settle \[6, 7\].

### 4\. Figure Out the Rule Book

Use the LLM to identify the governing technical framework or "rule book" for your specific domain \[7\]. This could involve finding the Medicare rules for hospital billing, the FDCPA and state statutes for debt collection, or IDEA and case law for special education services \[7\]. Knowing the applicable rule book(s) is essential, as you cannot audit compliance without it \[7\]. The AI should hunt up the correct, current copy of the rule book and use it to analyze your situation \[8\].

### 5\. Find True Categorical Violations

Focus on identifying clean, clear, binary violations of explicit rules rather than subjective complaints \[8\]. Institutions can safely ignore opinions (e.g., "my bill is too high" or "it seems expensive"), but they cannot defend a category violation (e.g., violating a CMS regulation by bundling codes separately) \[2, 8\]. This involves using the LLM to find where the specific facts of your situation break a category that exists within the rule book \[2\]. This step naturally builds on the previous step of finding and reading the applicable rule book \[2\].

### 6\. Calculate Objective Anchors from Authoritative Standards

Establish a defensible position by calculating objective anchors based on published standards \[9\]. Your argument must shift from subjective feelings (e.g., "I can't afford this") to documented standards (e.g., Medicare reimbursement rates, required clinical guidelines, or comparable property sales averages) \[9, 10\]. For instance, establishing what Medicare would reimburse for a procedure gives you a stronger position than a general complaint about the cost \[9\].

### 7\. AI Collapse Investigative Costs while Leaving You in Control of Verification

AI enables investigation at a truly institutional scale, collapsing the cost from thousands of dollars (for lawyers or advocates) down to potentially just a few hours of your time \[10-12\]. While the AI identifies potential violations, explains the violation, and drafts responses, **you must remain in control of verifying the findings that carry risk** \[10, 13\]. This verification step is necessary because the AI cannot take legal liability, and wrong citations in adversarial contexts signal a lack of knowledge \[10, 13\].

### 8\. Let AI Draft Verification Prompts to Catch Its Own Mistakes

To install safeguards in your institutional-grade investigation, use the AI itself to draft verification prompts \[12\]. This allows the AI to fact-check the dispute letter, flag citation errors, and check for incorrect interpretations of codes \[12\]. Although the AI assists in finding its own errors, the user remains ultimately responsible for the verification step \[12\].

---

**Underlying Principle:** A key understanding underlying this methodology is that **investigation must precede negotiation** \[14\]. By following these eight principles, the user gains frame control by moving the conversation away from seeking charity toward negotiating based on documented violations, putting the institution on weaker ground \[14\].

This detailed methodology turns the challenge of adversarial negotiation, where institutions rely on intentionally constructed complexity to maintain a monopoly on information \[3, 15\], into a structured process where the individual uses AI as a weapon to level the playing field \[1, 16\].