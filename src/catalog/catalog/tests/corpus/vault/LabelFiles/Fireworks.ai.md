# Fireworks.ai

[Inference.md](./Inference.md)

[Serverless.md](./Serverless.md)

[AI Compute Providers 2.md](./AI%20Compute%20Providers%202.md)

<https://fireworks.ai/>

> Fireworks partners with the world's leading generative AI researchers to serve the best models, at the fastest speeds.

>  anyone looking for fast inference without the smoke and mirrors, I've been using [Fireworks.ai](Fireworks.ai) in production and it's great. 200 tk/s - 300 tk/s is closer to Groq than it is to OpenAI and co.